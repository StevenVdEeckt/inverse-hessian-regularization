# Inverse-Hessian Regularization for Continual Learning in ASR
Supplementary material to the paper "Inverse-Hessian Regularization for Continual Learning in ASR", submitted at ICASSP 2026.

## results

Full results of the experiments can be found in `results/results.xlsx`. 

This is an excel file that contains four sheets:
1. `Exp 1 - CV` - contains all results for Experiment 1.
2. `Exp 2 - CV` - contains all results for Experiment 2.
3. `Significance Testing` - contains all results regarding significance testing.
4. `Hyperparam` contains all results regarding the hyper-parameter optimization.

Regarding the results for Experiment 1 and 2: 
- The results after each of the five adaptation can be found: WER per task, Average WER (AVG), Backward Transfer (BWT), Forward Transfer (FWT) and Coverage (COV). Note that for lack of space, FWT and COV are not mentioned in the paper. Using the notation used in the paper, Forward Transfer is defined as $\text{FWT}=\frac{1}{T-1} \sum_{k=2}^T (R_{1,k}-R_{k,k}$) with $R_{i,j}$ the WER on task $j$ using the model after $i$ tasks, thus with parameters $\mathbf{\theta}^i$. COV, on the other hand, is defined so that methods performing as poor as Fine-Tuning in terms of Average WER have $0\%$, while methods performing as well as Sep. Model have $100\%$. Sep. Model, which is not added to the paper, presents a best-case scenario, by storing a separate model per task and relying on a task oracle to determine which model to use at inference time.
- For Experiment 1, the results also contain the performance of the models tested for the ablation from Tab. 2 or for the analysis in Fig. 2.

Regarding the Significance Testing:
- Statistical significance of the results was tested by computing the number of errors per utterance [Strik et al., 2000] for each method and comparing two methods with Wilcoxon sign-rank test. The significance testing is thus applied to Average WER.
- Three levels of significance were considered: 0.05 (\*), 0.01 (\*\*) and 0.001 (\*\*\*).
- If the square is green, the method from the row significantly outperforms the method from the column; and vice-versa if the square is red. If the square is orange, there is no significant difference between the methods. 

## code

ESPnet2 [Wanatabe et al., 2018] is used for all experiments. 

Two continual learning extensions were added to ESPnet, which can be found in `espnet2/`:
1. `continual_learning2.py` implements the baselines ER, UOE and CLRL-T, which is then added to the model in `espnet2/asr/espnet_model.py`.
2. `consolidate2.py` implements the computation of the Kronecker-factored Hessian approximations. It iterates over all data from the previous task.

In addition, the merging step of FTA and IHR are implemented in `merging_step/compute_fta_model.py` and `merging_step/compute_ihr_model.py`, resp. These functions should be applied to the model after training (fine-tuning) on the new task.

## model

The model used in the experiments is an encoder-decoder end-to-end ASR model. It consists of a Conformer encoder and a Transformer decoder and has approx. 47M parameters. 

The configuration files for training can be found `models/training/`: 
- `initial_task/` contains the configuration file to train the initial model on task $1$.
- `continual_learning/` contains the configuration file to learn the remaining tasks $2, ..., T$, using the baselines or our method.

In addition, the configuration file for decoding can be found in `models/decoding/`

Note that the outputs of the model are 5000 word pieces generated by SentencePiece model [Kudo and Richardson, 2018] on the initial task.  

## data

Two experiments are conducted in the paper, for which the data (list of utterances, speakers, etc.) can be found in [our other repository](https://github.com/StevenVdEeckt/efficient-rehearsal-for-cl-in-asr). This includes the utterances stored in the memory for the baseline ER. 

## hyper-parameters

Our method, IHR, contains a hyper-parameter $\tau$ for which we determine the optimal value on the first adaptation of each experiment, using the validation sets to evaluate. For the baseline, ER, too, a weight $\lambda$ of its regularization loss (following [Vander Eeckt and Van hamme, 2022]) must be determined. 

For Experiment 1, the table below summarizes the hyper-parameters per method, the tested values, and the default and optimal (both with Average WER between brackets):

method  | hyper-parameters | values tried | default | optimal
------------- | ------------- | ------------- | ------------- | ------------- 
ER | $\lambda$ | $(0.1, 1.0)$ | $0.1$ $[16.00]$ | $0.1$ $[16.00]$
AOS (ours) | $\tau$ | $(1.0, 2.0, 3.0, 5.0)$ | $1.0$ $[16.11]$ | $1.0$ $[16.11]$

For Experiment 2, the table below summarizes the hyper-parameters per method, the tested values, and the default and optimal (both with Average WER between brackets):

method  | hyper-parameters | values tried | default | optimal
------------- | ------------- | ------------- | ------------- | ------------- 
ER | $\lambda$ | $(0.1, 1.0)$ | $0.1$ $[4.54]$ | $1.0$ $[4.44]$
AOS (ours) | $\tau$ | $(1.0, 2.0, 3.0, 5.0)$ | $1.0$ $[6.30]$ | $3.0$ $[4.69]$

Note, moreover, that full results regarding the hyper-parameters can be found in `results/results.xlsx`

## References 

[Kudo and Richardson, 2018] Taku Kudo and John Richardson. SentencePiece: A simple and language independentsubword tokenizer and detokenizer for neural text processing. InProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 66–71, Brussels, Belgium, November 2018. Association for Computational Linguistics.

[Strik et al., 2000] Helmer Strik, Catia Cucchiarini, and Judith M. Kessens, “Comparing the recognition performance of csrs: in search of an adequate metric and statistical significance test,” in INTERSPEECH, 2000.

[Vander Eeckt and Van hamme, 2022] S. Vander Eeckt and H. Van hamme, “Continual learning for monolingual end-to-end automatic speech recognition,” Proceedings EUSIPCO 2022, 2022.

[Watanabe et al., 2018] S. Watanabe et al., “ESPnet: End-to-end speech processing toolkit,” in Proceedings of Interspeech, 2018, pp. 2207–2211. 


